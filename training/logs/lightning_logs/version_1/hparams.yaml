accelerator: gpu
batch_size: 32
default_root_dir: training/logs
devices: '1'
fast_dev_run: false
hidden_dropout_prob: 0.1
hidden_size: 768
learning_rate: 2.0e-05
lora_alpha: 1.0
lora_rank: 8
max_epochs: 5
max_length: 256
min_epochs: 1
model_name: bert
num_workers: 4
precision: 32-true
seed: 42
strategy: auto
test_split: 0.1
use_lora: false
val_split: 0.2
wandb: false
weight_decay: 0.01
