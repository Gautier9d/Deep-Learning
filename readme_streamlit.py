import streamlit as st
import os

def render_readme():
    st.title("DeepDog: Deep Learning for Dog Whistle Detection")
    st.markdown("EPFL's EE-559 - Deep Learning Project Repository")
    
    # Display badges in a single line
    badges = [
        '[![Open Source Love](https://firstcontributions.github.io/open-source-badges/badges/open-source-v2/open-source.svg)](https://github.com/firstcontributions/open-source-badges)',
        '[![License: MIT](https://img.shields.io/badge/License-MIT-red.svg)](https://opensource.org/licenses/MIT)',
        '[![GitHub](https://img.shields.io/badge/GitHub-%23121011.svg?logo=github&logoColor=white)](https://github.com/Gautier9d/DeepDog)',
        '[![Streamlit App](https://static.streamlit.io/badges/streamlit_badge_black_white.svg)](https://deepdog.streamlit.app/)'
    ]
    st.markdown(' '.join(badges), unsafe_allow_html=True)
    
    st.image(os.path.join("assets", "banner.png"), use_container_width=True)
    st.caption("Image generated by ChatGPT")

    st.warning("⚠️ WARNING: This repository contains content that are offensive and/or hateful in nature as part of the research dataset and examples.")

    st.header("Abstract")
    st.markdown("""
We present DeepDog, a transformer-based framework for detecting and analyzing coded language ("dog whistles") in online content. The framework supports multiple pre-trained models (BERT, DistilBERT, HateBERT, and HateXplain) and implements parameter-efficient fine-tuning through Low-Rank Adaptation (LoRA). Evaluated on the SALT-NLP Silent Signals dataset, the models we used demonstrate robust performance in identifying dog whistle. Additionally, the project includes environmental impact tracking, visualizing the carbon footprint of model training in terms of real-world equivalents.
    """)

    st.header("The Challenge of Dog Whistle Detection")
    st.markdown("The following example demonstrates the complexity and subtlety involved in detecting dog whistles in text. Two different LLMs when asked the same question yield contradicting interpretations, highlighting why advanced ML techniques are necessary for this task:")
    col1, col2 = st.columns(2)
    with col1:
        st.image(os.path.join("assets", "grok_output.png"), caption="Grok: Identifies the text as containing coded language", use_container_width=True)
    with col2:
        st.image(os.path.join("assets", "chatgpt_output.png"), caption="ChatGPT: Labels the same text as innocuous", use_container_width=True)

    st.header("Key Features")
    st.markdown("""
- **Multi-Model Support**: Compatible with BERT, DistilBERT, HateBERT, and HateXplain
- **Efficient Fine-tuning**: Implements LoRA for parameter-efficient model adaptation
- **Environmental Tracking**: Monitors and reports carbon emissions during training
- **Comprehensive Metrics**: Evaluates using IOU, F1 Score, and AUPRC metrics
    """)

    st.header("Environmental Impact")
    st.markdown("The project includes built-in carbon emission tracking using [codecarbon](https://github.com/mlco2/codecarbon). Training results include visualizations of:")
    st.markdown("- Equivalent car miles driven\n- Percentage of weekly American household emissions")

    st.header("Experiment Tracking")
    st.markdown("All experiments and model artifacts are tracked using Weights & Biases (W&B). You can explore our training runs, model performance, and artifacts at: [https://wandb.ai/gopald/deep-dog](https://wandb.ai/gopald/deep-dog)")

    st.header("Course Information and Contributors")
    st.markdown("""
- **Course**: EE-559 - Deep Learning
- **Teacher**: Cavallaro Andrea
- **Group**: Romain Nicolas Paul Couyoumtzelis, Gopal Ramesh Dahale, Gautier Demierre
    """)

    st.header("References")
    st.markdown("""
1. Silent Signals, Loud Impact: LLMs for Word-Sense Disambiguation of Coded Dog Whistles  
   Julia Kruk, et al.  
   [arXiv:2406.06840](https://arxiv.org/abs/2406.06840)
2. LoRA: Low-Rank Adaptation of Large Language Models  
   Edward J. Hu, et al.  
   [arXiv:2106.09685](https://arxiv.org/abs/2106.09685)
3. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding  
   Jacob Devlin, et al.  
   [arXiv:1810.04805](https://arxiv.org/abs/1810.04805)
4. DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter  
   Victor Sanh, et al.  
   [arXiv:1910.01108](https://arxiv.org/abs/1910.01108)
5. HateBERT: Retraining BERT for Abusive Language Detection in English  
   Tommy Liu, et al.  
   [arXiv:2010.12472](https://arxiv.org/abs/2010.12472)
    """)

    st.header("Acknowledgments")
    st.markdown("""
- Project structure and code quality standards were inspired by [Full Stack Deep Learning](https://github.com/the-full-stack/fsdl-text-recognizer-2021-labs) course materials.
- Parts of this project's code were developed with assistance from Large Language Models (LLMs). This way, we want to promote code quality and development efficiency while maintaining transparency about the tools.
    """)

    st.header("Citation")
    st.markdown("If you use this code in your research, please cite:")
    st.code("""@misc{deepdog2025,
    title={DeepDog: Deep Learning for Dog Whistle Detection},
    author={Romain Nicolas Paul Couyoumtzelis, Gopal Ramesh Dahale, Gautier Demierre},
    year={2025},
    publisher={GitHub},
    journal={GitHub repository},
    howpublished={https://github.com/Gautier9d/DeepDog}
}""", language="bibtex")
